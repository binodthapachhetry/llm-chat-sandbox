# llm-chat-sandbox
Minimal Node.js chat app for experimenting with open-weight LLM inference (Ollama, vLLM, TGI) and evaluating scaling, caching, and streaming strategies for future FedRAMP-compliant deployment.
