PORT=3000
# One of: ollama | openai_compat
PROVIDER=openai_compat

# If PROVIDER=openai_compat:
LLM_BASE_URL=

LLM_API_KEY=sk-not-needed-for-local # optional; many self-hosts ignore it

# Default models per provider
OLLAMA_MODEL=gemma3:12b-it-qat
OPENAI_COMPAT_MODEL=unsloth/gemma-3-4b-it-unsloth-bnb-4bit
